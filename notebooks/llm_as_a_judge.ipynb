{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c1f14c5-bbc5-4943-8e71-440de0705341",
   "metadata": {},
   "source": [
    "# LLM-as-a-Judge\n",
    "\n",
    "This notebook showcases the different scenarios of LLM-as-a-Judge with concrete examples.\n",
    "\n",
    "Its main purpose is to create alignment and structure the work that needs to be done to integrate AI-assisted judgments into the Search Relevance Workbench.\n",
    "\n",
    "The different use cases where judgments are generated with language models:\n",
    "\n",
    "1. The classic approach: Users define a judgment generation process by defining a query set, a LLM and a prompt. For every query in the query set the top n documents are retrieved by a search configuration and for each query-doc pair the LLM generates a judgment together with a reasoning statement.\n",
    "2. The classic approach embedded in an experiment: Users run an experiment and want to create judgments \"on-the-fly\" by referencing an empty judgment list. The process of judgment.generation is identical to the first option. The difference is the user journey: create an empty list, run an experiment including judgment generation as part of the overarching process.\n",
    "3. Filling in gaps: Users run an experiment and want to fill in any gaps. It is common to not have all query-doc pairs judged. With LLM-as-a-judge these gaps can be filled on the fly\n",
    "4. Similarity-based judgments: Users want to generate judgments with LLMs that are based on the similarity of a provided reference statement to a retrieved document."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76470d98-9519-42f7-b7fe-c213fc48fa8d",
   "metadata": {},
   "source": [
    "## Notebook requirements\n",
    "\n",
    "* Python 3\n",
    "* Follow the instructions in `README.md` to set up a virtual environment and install the required Python libraries\n",
    "* Ollama is used as the local model serving engine\n",
    "* The [Gemma 3](https://ollama.com/library/gemma3) model is used for LLM-based judgments. Download the model with `ollama run gemma3`\n",
    "* [All-MiniLM](https://ollama.com/library/all-minilm) is used for the similarity-based judgments. Download the model with `ollama pull all-minilm`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16d737f-6691-43d2-affb-663c1357c7a7",
   "metadata": {},
   "source": [
    "### Query Set and Documents\n",
    "\n",
    "We assume a simple set of three queries and have three documents.\n",
    "\n",
    "Three example documents are taken from the [ESCI dataset](https://github.com/amazon-science/esci-data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "f5833ccf-5067-4850-b063-d4989ee633df",
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = [\"barbie\", \"airpods\", \"apple airpods\"]\n",
    "documents = [\n",
    "    {\n",
    "        \"id\": \"1\",\n",
    "        \"abstract\": \"\",\n",
    "        \"title\": \"Barbie Malibu House Playset 60 cm\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"2\",\n",
    "        \"abstract\": \"Airpods Case - Airspo 7 in 1 Airpods Accessories Set Compatible with Airpods 1 & 2\",\n",
    "        \"title\": \"Airpods Case - Airspo 7 in 1 Airpods Accessories Set Compatible with Airpods 1 & 2 Protective Silicone Cover Floral Print Cute Case (Black Rose)\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"3\",\n",
    "        \"abstract\": \"Welcome to the World's First Apple Cider Vinegar Gummies! Vegan, Gluten Free, Non-GMO and made with the highest quality ingredients! Our patented formula combines Apple Cider Vinegar, traditionally used as a remedy for digestion, gut health, and appetite, plus essential Vitamins B9 and B12 that support overall good health. Taste the Apple. Not the Vinegar.\",\n",
    "        \"title\": \"\"\"Products Apple Cider Vinegar Gummy Vitamins by Goli Nutrition - 3 Pack - (180 Count, Organic, Vegan, Gluten-Free, Non-GMO, with\"The Mother\", Vitamin B9, B12, Beetroot, Pomegranate)\"\"\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be872a38-ce32-46ac-a7fe-43c226ad00ac",
   "metadata": {},
   "source": [
    "### Query-Doc Pairs\n",
    "\n",
    "We combine every document with each query to have a list of 9 query-doc pairs.\n",
    "\n",
    "We structure these in the format of a user message that we can pass to a instruction-based LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "12335a8a-5720-49ea-a2f5-10fc51684c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_messages = []\n",
    "\n",
    "for query in queries:\n",
    "    for doc in documents:\n",
    "        user_message = {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"\"\"\n",
    "            Query: {query}\n",
    "            doc1:\n",
    "                title: {doc['title']}\n",
    "                abstract: {doc['abstract']}\n",
    "            \"\"\"\n",
    "        }\n",
    "        user_messages.append(user_message)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ed9f29-4af1-420e-ad4b-098ac7e340b1",
   "metadata": {},
   "source": [
    "## Use Cases 1 & 2 - The Classic LLM-as-a-Judge Scenario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "bee175ee-28f0-4a7a-af00-fa452d081688",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "07e72fe9-6ae3-43a2-a0c3-fe451bcd61d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ollama import chat\n",
    "from ollama import ChatResponse\n",
    "\n",
    "# We create a prompt that consists of system message and user message.\n",
    "\n",
    "# The system prompt contains the instructions for the LLM. These contain an explanation of the judgment scale,\n",
    "# the output format and examples.\n",
    "system_message = {\n",
    "    \"role\": \"system\",\n",
    "    \"content\": \"\"\"You are evaluating the results from a search engine. For each query, you will be provided with multiple documents. Your task is to evaluate each document and assign a judgment on a scale of 0 to 3, where:\n",
    "    - 0 indicates the document is irrelevant to the query.\n",
    "    - 1 indicates the document is somewhat relevant to the query.\n",
    "    - 2 indicates the document is mostly relevant to the query.\n",
    "    - 3 indicates the document is perfectly relevant to the query.\n",
    "\n",
    "    For each document, provide:\n",
    "    1. An explanation of the judgment.\n",
    "    2. The judgment value.\n",
    "\n",
    "    The response should be in the following JSON format:\n",
    "    {\n",
    "      \"explanation\": \"Your detailed reasoning behind the judgment\",\n",
    "      \"judgment\": <numeric value>\n",
    "    }\n",
    "\n",
    "    Here are three examples:\n",
    "    User:\n",
    "    Query: Farm animals\n",
    "\n",
    "    doc1:\n",
    "      title: All about farm animals\n",
    "      abstract: This document is all about farm animals\n",
    "    Assistant:\n",
    "    {\n",
    "      \"explanation\": \"This document appears to perfectly respond to the user's query\",\n",
    "      \"judgment\": 3\n",
    "    }\n",
    "\n",
    "    User:\n",
    "    Query: Farm animals\n",
    "\n",
    "    doc2:\n",
    "      title: Somewhat about farm animals\n",
    "      abstract: This document somewhat talks about farm animals\n",
    "    Assistant:\n",
    "    {\n",
    "      \"explanation\": \"This document is somewhat relevant to the user's query\",\n",
    "      \"judgment\": 1\n",
    "    }\n",
    "\n",
    "    User:\n",
    "    Query: Farm animals\n",
    "\n",
    "    doc3:\n",
    "      title: This document has nothing to do with farm animals\n",
    "      abstract: We will talk about everything except for farm animals.\n",
    "    Assistant:\n",
    "    {\n",
    "      \"explanation\": \"This document is not relevant at all to the user's query\",\n",
    "      \"judgment\": 0\n",
    "    }\"\"\"\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "ed4a89bb-677e-40de-a2d0-2891b53c5c67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "  \"explanation\": \"This document is highly relevant to the query 'barbie'. The title explicitly mentions 'Barbie' and the abstract refers to a Barbie product (the Malibu House Playset).\",\n",
      "  \"judgment\": 3\n",
      "}\n",
      "```\n",
      "```json\n",
      "{\n",
      "  \"explanation\": \"This document is completely irrelevant to the query 'barbie'. It discusses Airpods accessories and has nothing to do with the Barbie brand or the topic of Barbie.\",\n",
      "  \"judgment\": 0\n",
      "}\n",
      "```\n",
      "```json\n",
      "{\n",
      "  \"explanation\": \"This document discusses apple cider vinegar gummies, which has absolutely no connection to the query 'barbie'. It's entirely irrelevant.\",\n",
      "  \"judgment\": 0\n",
      "}\n",
      "```\n",
      "```json\n",
      "{\n",
      "  \"explanation\": \"This document is completely irrelevant to the query 'airpods'. It discusses a Barbie play set and has no connection to the topic of wireless earbuds.\",\n",
      "  \"judgment\": 0\n",
      "}\n",
      "```\n",
      "```json\n",
      "{\n",
      "  \"explanation\": \"This document is highly relevant to the query 'airpods'. It directly mentions 'airpods' in the title and abstract, and describes a case specifically designed for them. Itâ€™s a very specific and relevant product description.\",\n",
      "  \"judgment\": 3\n",
      "}\n",
      "```\n",
      "```json\n",
      "{\n",
      "  \"explanation\": \"This document discusses apple cider vinegar gummies, completely unrelated to the user's query for \\\"airpods\\\". It does not mention headphones or audio devices.\",\n",
      "  \"judgment\": 0\n",
      "}\n",
      "```\n",
      "```json\n",
      "{\n",
      "  \"explanation\": \"This document is completely irrelevant to the query 'apple airpods'. It discusses a Barbie toy, while the query is about Apple's wireless headphones.\",\n",
      "  \"judgment\": 0\n",
      "}\n",
      "```\n",
      "```json\n",
      "{\n",
      "  \"explanation\": \"This document directly addresses the query 'apple airpods' by mentioning 'Airpods'. The title and abstract explicitly refer to Airpods accessories, indicating a strong connection to the user's search.\",\n",
      "  \"judgment\": 3\n",
      "}\n",
      "```\n",
      "```json\n",
      "{\n",
      "  \"explanation\": \"This document discusses apple cider vinegar gummies, not Apple AirPods. It completely misses the query's focus.\",\n",
      "  \"judgment\": 0\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "# We pass the system message and one user message at a time to the LLM and retrieve the answer\n",
    "for msg in user_messages:\n",
    "    response: ChatResponse = chat(model='gemma3', messages=[\n",
    "      system_message, msg\n",
    "    ])\n",
    "    print(response['message']['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "079d4a57-bcad-4f5e-96ee-26be34d4f1b9",
   "metadata": {},
   "source": [
    "## Use Case 3 - Filling the Gaps\n",
    "\n",
    "The LLM-as-a-Judge process only requests judgments for unrated query-doc pairs.\n",
    "\n",
    "The basic process behind judging stays the same."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f4b370-79b1-4516-98ab-0f80812c4b84",
   "metadata": {},
   "source": [
    "### Judgments\n",
    "\n",
    "We now assume a judgment list that contains judgments for some of the query-doc pairs, not all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "223208d2-442e-45e3-ba11-4bfcf61b5cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "judgments = [\n",
    "    {\n",
    "        \"query\": \"barbie\",\n",
    "        \"doc\": \"1\",\n",
    "        \"judgment\": 1\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"barbie\",\n",
    "        \"doc\": \"3\",\n",
    "        \"judgment\": 0\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"airpods\",\n",
    "        \"doc\": \"3\",\n",
    "        \"judgment\": 0\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "a5a80658-20c6-47aa-a0b7-bf32064b979d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We now create user messages only for unrated query-doc pairs\n",
    "user_messages = []\n",
    "\n",
    "for query in queries:\n",
    "    for idx, doc in enumerate(documents, start=1):\n",
    "        if any(j[\"query\"] == query and j[\"doc\"] == str(idx) for j in judgments):\n",
    "            continue  # Skip if a judgment exists\n",
    "        \n",
    "        user_message = {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"\"\"\n",
    "            Query: {query}\n",
    "            doc1:\n",
    "                title: {doc['title']}\n",
    "                abstract: {doc['abstract']}\n",
    "            \"\"\"\n",
    "        }\n",
    "        user_messages.append(user_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "78d9013a-2934-4f18-a870-03fd164d2496",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "  \"explanation\": \"This document is completely irrelevant to the query 'barbie'. It discusses Airpods accessories and has nothing to do with the famous doll.\",\n",
      "  \"judgment\": 0\n",
      "}\n",
      "```\n",
      "```json\n",
      "{\n",
      "  \"explanation\": \"This document is completely irrelevant to the query 'airpods'. It discusses a Barbie toy house and has no connection to the topic of wireless earbuds.\",\n",
      "  \"judgment\": 0\n",
      "}\n",
      "```\n",
      "```json\n",
      "{\n",
      "  \"explanation\": \"This document is highly relevant to the query 'airpods'. It specifically mentions 'Airpods' in the title and abstract, detailing a case accessory for Airpods.\",\n",
      "  \"judgment\": 3\n",
      "}\n",
      "```\n",
      "```json\n",
      "{\n",
      "  \"explanation\": \"This document is completely irrelevant to the user's query about 'apple airpods'. It discusses a Barbie Malibu House Playset and has no connection to the topic.\",\n",
      "  \"judgment\": 0\n",
      "}\n",
      "```\n",
      "```json\n",
      "{\n",
      "  \"explanation\": \"This document is highly relevant as it directly addresses the query 'apple airpods' by discussing an 'Airpods' accessory set. While it focuses on a case, it's still related to the product the user is searching for.\",\n",
      "  \"judgment\": 3\n",
      "}\n",
      "```\n",
      "```json\n",
      "{\n",
      "  \"explanation\": \"This document discusses apple cider vinegar gummies, not Apple AirPods. It completely misses the query's focus.\",\n",
      "  \"judgment\": 0\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "# We pass the system message and one user message at a time to the LLM and retrieve the answer\n",
    "for msg in user_messages:\n",
    "    response: ChatResponse = chat(model='gemma3', messages=[\n",
    "      system_message, msg\n",
    "    ])\n",
    "    print(response['message']['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d34708-c616-4c60-8485-b04b35c1ffdf",
   "metadata": {},
   "source": [
    "## Use Case 4 - Similarity-based Judgments\n",
    "\n",
    "For Similarity-based Judgments we do not assume an instruction-based LLM to act as a judge but rather measure the similarity of a returned document to the query or to a provided reference statement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efbc8170-619b-42b1-9ce6-ae9a6b712210",
   "metadata": {},
   "source": [
    "### Query-to-Doc Similarity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "655467c0-4f38-43dd-a311-740e8c233315",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "8cd818f1-c84e-4195-952a-59ea33f2b1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#query = \"barbie\"\n",
    "#document = \"Barbie Malibu House Playset 60 cm\"\n",
    "query = \"apple airpods\"\n",
    "document = \"wireless earbuds\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "554357f7-fe23-426c-9030-c65f4d7597d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_embedding = ollama.embeddings(model='all-minilm', prompt=query)[\"embedding\"]\n",
    "document_embedding = ollama.embeddings(model='all-minilm', prompt=document)[\"embedding\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "6bfb4091-80ed-47d1-8c2a-c1cab388c97d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.43989050572567057\n"
     ]
    }
   ],
   "source": [
    "similarity = cosine_similarity([query_embedding], [document_embedding])[0][0]\n",
    "print(similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "cf20d661-e494-4d97-8687-092bee0d1979",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarities = []\n",
    "for query in queries:\n",
    "    for doc in documents:\n",
    "        query_embedding = ollama.embeddings(model='all-minilm', prompt=query)[\"embedding\"]\n",
    "        doc_text = \" \".join([str(doc['title']),str(doc['abstract'])])\n",
    "        document_embedding = ollama.embeddings(model='all-minilm', prompt=doc_text)[\"embedding\"]\n",
    "        similarity = cosine_similarity([query_embedding], [document_embedding])[0][0]\n",
    "        similarities.append({\"query\": query, \"document\": doc['title'], \"similarity\": similarity})        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "f135189e-1418-439a-bcb8-f96443585dfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "barbie\n",
      "Barbie Malibu House Playset 60 cm\n",
      "0.5484308971684816\n",
      "\n",
      "barbie\n",
      "Airpods Case - Airspo 7 in 1 Airpods Accessories Set Compatible with Airpods 1 & 2 Protective Silicone Cover Floral Print Cute Case (Black Rose)\n",
      "0.12998155612524992\n",
      "\n",
      "barbie\n",
      "Products Apple Cider Vinegar Gummy Vitamins by Goli Nutrition - 3 Pack - (180 Count, Organic, Vegan, Gluten-Free, Non-GMO, with\"The Mother\", Vitamin B9, B12, Beetroot, Pomegranate)\n",
      "0.025337382598659565\n",
      "\n",
      "airpods\n",
      "Barbie Malibu House Playset 60 cm\n",
      "0.0606708010043219\n",
      "\n",
      "airpods\n",
      "Airpods Case - Airspo 7 in 1 Airpods Accessories Set Compatible with Airpods 1 & 2 Protective Silicone Cover Floral Print Cute Case (Black Rose)\n",
      "0.6365950399555693\n",
      "\n",
      "airpods\n",
      "Products Apple Cider Vinegar Gummy Vitamins by Goli Nutrition - 3 Pack - (180 Count, Organic, Vegan, Gluten-Free, Non-GMO, with\"The Mother\", Vitamin B9, B12, Beetroot, Pomegranate)\n",
      "-0.08574967492363803\n",
      "\n",
      "apple airpods\n",
      "Barbie Malibu House Playset 60 cm\n",
      "0.06269648046198087\n",
      "\n",
      "apple airpods\n",
      "Airpods Case - Airspo 7 in 1 Airpods Accessories Set Compatible with Airpods 1 & 2 Protective Silicone Cover Floral Print Cute Case (Black Rose)\n",
      "0.6082166420340687\n",
      "\n",
      "apple airpods\n",
      "Products Apple Cider Vinegar Gummy Vitamins by Goli Nutrition - 3 Pack - (180 Count, Organic, Vegan, Gluten-Free, Non-GMO, with\"The Mother\", Vitamin B9, B12, Beetroot, Pomegranate)\n",
      "-0.033120692519209176\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def print_json_values(json_array):\n",
    "    for obj in json_array:\n",
    "        values = []\n",
    "        for key, value in obj.items():\n",
    "            if isinstance(value, list):\n",
    "                values.append(\"\\n\".join(map(str, value)))  # Join list elements with tab\n",
    "            else:\n",
    "                values.append(str(value))\n",
    "        print(\"\\n\".join(values) + \"\\n\")  # Print values separated by tabs\n",
    "\n",
    "print_json_values(similarities)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e830e69-63ba-4b96-9e91-fdeefa6e6294",
   "metadata": {},
   "source": [
    "### Reference-to-Doc Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "123e4431-32ee-4edf-840b-7a1f888b2857",
   "metadata": {},
   "source": [
    "Reference statements are more common in Question-Answering or similar systems.\n",
    "\n",
    "For this scenario we create an additional query set that contains a reference for each query.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "7b4171e0-6f0c-4c9d-9c2b-488607c60441",
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = [\n",
    "    {\n",
    "        \"query\": \"What's the captial of Germany\",\n",
    "        \"reference\": \"Berlin is the capital of Germany.\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"who was the 30th president of the united states\",  \n",
    "        \"reference\": \"Calvin Coolidge was America's 30th president from 1923-1929.\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"How many books are in the Harry Potter series\",\n",
    "        \"reference\": \"There are 7 books in the Harry Potter series.\"\n",
    "    }\n",
    "]\n",
    "documents = [\n",
    "    {\n",
    "        \"id\": \"1\",\n",
    "        \"title\": \"Bonn\",\n",
    "        \"abstract\": \"Before Berlin, Bonn was the capital of capital of Germany from 1949 to 1990,\",\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"2\",\n",
    "        \"title\": \"Harry Potter Fandom\",\n",
    "        \"abstract\": \"8 Harry Potter movies exist.\",\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"3\",\n",
    "        \"title\": \"Calvin Coolidge\",\n",
    "        \"abstract\": \"In 1872 Calvin Coolidge was born in Plymouth Notch. In 1923 he became America's 30th president.\",\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "df95e448-ea32-407a-aa88-f1413b91e2d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarities = []\n",
    "for query in queries:\n",
    "    reference_text = query['reference']\n",
    "    for doc in documents:\n",
    "        reference_embedding = ollama.embeddings(model='all-minilm', prompt=reference_text)[\"embedding\"]\n",
    "        doc_text = \" \".join([str(doc['title']),str(doc['abstract'])])\n",
    "        document_embedding = ollama.embeddings(model='all-minilm', prompt=doc_text)[\"embedding\"]\n",
    "        similarity = cosine_similarity([reference_embedding], [document_embedding])[0][0]\n",
    "        similarities.append({\"query\": query['query'], \"reference\": reference_text, \"document\": doc['title'], \"similarity\": similarity})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "d4ae1561-6bd3-4f94-b6c6-d9ccf297ef5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What's the captial of Germany\n",
      "Berlin is the capital of Germany.\n",
      "Bonn\n",
      "0.6175566088869178\n",
      "\n",
      "What's the captial of Germany\n",
      "Berlin is the capital of Germany.\n",
      "Harry Potter Fandom\n",
      "-0.04672439933953921\n",
      "\n",
      "What's the captial of Germany\n",
      "Berlin is the capital of Germany.\n",
      "Calvin Coolidge\n",
      "0.015167892858684733\n",
      "\n",
      "who was the 30th president of the united states\n",
      "Calvin Coolidge was America's 30th president from 1923-1929.\n",
      "Bonn\n",
      "0.11039106903291925\n",
      "\n",
      "who was the 30th president of the united states\n",
      "Calvin Coolidge was America's 30th president from 1923-1929.\n",
      "Harry Potter Fandom\n",
      "-0.038401802991445425\n",
      "\n",
      "who was the 30th president of the united states\n",
      "Calvin Coolidge was America's 30th president from 1923-1929.\n",
      "Calvin Coolidge\n",
      "0.7403091215042535\n",
      "\n",
      "How many books are in the Harry Potter series\n",
      "There are 7 books in the Harry Potter series.\n",
      "Bonn\n",
      "-0.11268842032783113\n",
      "\n",
      "How many books are in the Harry Potter series\n",
      "There are 7 books in the Harry Potter series.\n",
      "Harry Potter Fandom\n",
      "0.6414299004788524\n",
      "\n",
      "How many books are in the Harry Potter series\n",
      "There are 7 books in the Harry Potter series.\n",
      "Calvin Coolidge\n",
      "-0.09833503301868894\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_json_values(similarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2498ff02-0d0c-478f-bae2-92edac51740a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
